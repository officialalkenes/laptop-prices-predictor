# -*- coding: utf-8 -*-
"""laptop_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v0cxHDv75UCQbI33bBkFcC1XY26358lg

### CETM46 â€“ Data Science Product Development Patrt 2

---



---



---



---

## **1.Introduction**

## Application Domain: 

Computer hardware industry or the consumer electronics industry

## Problem Statement

This notebook conduct trend analysis, forecasting, and recommendation to provide insights to customers and manufacturers on the performance, features, and prices of laptops. 
<br>

The dataset will contain information such as the brand, model, specifications, prices, and ratings of laptops. 

### Goals
- The goal is to analyze this information to identify patterns and trends in the market, forecast future demand.
- provide recommendations to customers on the best laptops to purchase based on their needs and budget. 
- The dataset will be used by manufacturers to gain insights into the market and make informed decisions about product development and pricing strategies.



## Data Attribution
* Data preparation steps required to clean and prepare the data for analysis. This would include data cleaning, data transformation, and data integration.

* Data visualization techniques including charts, graphs and how they can be used to gain insights into sales performance metrics.

* Exploratory Data Analysis: This section would cover various aspects of data analysis, such as visualizing the data, identifying patterns, and summarizing key statistics.

# Table of Content : 

[1. Installing Library](#1.Installation)

[2. Importing Library](#2.Importing_Libraries_and_Packages)

[3. Data Collection](#3.Data_Collection) 

[4. Data Inspection and Validation](#4.Data_Inspection_and_Validation) 

[5. Data Appropriations](#5.Data_Approriation) 

[6. Data Visualisation ](#6.Data_Visualisation) 

[7. Conclusion](#7.Conclusion)

[8. Reflection](#8.Reflection)

## An extensive dataset of laptops for trend analysis, forecasting & Recommendation

**Objectives** 

- To analyze the trends in the laptop industry and identify the key factors that influence consumer preferences and purchase decisions.
- To develop a forecasting model that can predict future demand for laptops based on historical sales data and market trends.
- To recommend specific laptop brands and models to consumers based on their needs and preferences.
- To provide insights to laptop manufacturers on how they can improve their product offerings and marketing strategies to better meet consumer demands.
- To contribute to the existing body of knowledge in the field of data analytics and machine learning by demonstrating the application of these techniques in the laptop industry.
- To generate a reproducible notebook, to evaluate the store sales performance and other key features influencing the sales for the given period.

# 1.Installation

In this Notebook we are using [Pandas](https://pandas.pydata.org) to process the dataset, [Plotly Express](https://plotly.com/python/plotly-express/) and [Matplotlib](https://matplotlib.org/) to produce the visualisations. Other packages will also use for the analysis are stated below.
"""

!pip install pandas
!pip install numpy
!pip install matplotlib
!pip install seaborn
!pip install plotly_express
!pip install sklearn

"""# 2. Importing the necessary packages"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_squared_error, r2_score

"""# 3. Load Csv

### The csv file `Laptop_Information.csv` for this dataset is stored in the root directory.

- read as csv and store the value in the variable name `df`. which is an abbreviation for `DataFrame`
"""

df = pd.read_csv('laptop_price.csv', encoding='latin-1')

df

df.sample(3)

"""The code above `df.sample(3)` is a Pandas method that returns a random sample of 3 rows from a DataFrame df. This method is useful when we want to quickly view a random subset of the data to get a sense of its overall structure or to perform exploratory data analysis.

# 4. Data Inspection
"""

df.columns

"""`df.columns` returned value is a list of strings representing the column names in the order they appear in the DataFrame. This is helpful in inspecting the columns and renaming in Wrongly labelled columns.

From inspection, we can easily relate with all column names. There won't be need for renaming the columns.
"""

df.shape

"""`df.shape` helps to inspect the rows and columns stored in our `df`. The data contained in `df` contained `984` rows and `13` columns."""

df.info()

df.duplicated().sum()

df.isnull().sum()

"""`df.duplicated().sum()` returns 0.

The advantage of checking for duplicated rows is that it helps ensure the quality and integrity of the data. Duplicate rows can arise from various sources such as human error during data entry or data scraping, data merging, or data storage. Duplicates can introduce bias into the analysis, skewing the results and leading to inaccurate conclusions. Therefore, identifying and removing duplicates is an important step in data cleaning and preprocessing.

In summary, df.duplicated().sum() is a simple and quick method to check for duplicates in the data, and its output can be used to ensure the quality and integrity of the data and to improve the accuracy of the analysis.

Upon inspection, it returns 0 which is a clear indication it is clear indication there is no duplicate across the rows

<br>

Unlike the `df.duplicated().sum()`, `df.isnull().sum()` is a Pandas method that returns the number of missing or null values in each column of a DataFrame df. The advantage of this method is that it helps identify the amount and location of missing data in the data set. Missing data can be a result of various factors such as incomplete data entry, data corruption, or missing values in the original data source. Identifying and handling missing data is important in data analysis because it can affect the results and conclusions drawn from the data.

 <br>

In summary, while both df.isnull().sum() and df.duplicated().sum() are useful methods in EDA, they serve different purposes. df.isnull().sum() helps identify missing data, and df.duplicated().sum() helps identify and remove duplicate data.

# 4. Data Appropriation/ Cleaning
"""

df.drop(columns=['laptop_ID','Product'],inplace=True)

"""The `laptop_ID and Product` columns are not a necessary feature for prediction. Due to multitudes of random Information."""

#Now remove GB and kg in 'Ram' and 'Weight' columns
df['Ram']=df['Ram'].str.replace('GB','')
df['Weight']=df['Weight'].str.replace('kg','')

#convert the 'Ram' and 'Weight' columns object to Int and float
df['Ram']=df['Ram'].astype('int32')
df['Weight']=df['Weight'].astype('float32')

"""The `Ram and Weight` columns was stripped off the non-numeral characters and converted to the appropriate numeral data types `(int and float)` respectively.

# 3. Data Visualization and Analysis
"""

#Data Analysis 
fig = px.histogram(df, x='Price_euros')
fig.show()

"""From the graph above, There is Laptop prices of range 400-500 are the most available and trending laptops. Higher Prices is a sign of low trend in the dataset"""

df['Company'].value_counts().plot(kind='bar')

"""Upon Inspection, Dell and Lenovo Laptops are more Popular than any other Company.

With the Huawei Laptop having the lowest trend.
"""

sns.barplot(x=df['Company'],y=df['Price_euros'])
plt.xticks(rotation='vertical')
plt.show()



#now TypeName
df['TypeName'].value_counts().plot(kind='bar')

#with price
sns.barplot(x=df['TypeName'],y=df['Price_euros'])
plt.xticks(rotation='vertical')
plt.show()

"""NoteBooks are the most Popular, while Workstation are more expensive when tallied on the average price"""

fig = px.histogram(df, x='Inches')
fig.show()

#with price
sns.scatterplot(x=df['Inches'],y=df['Price_euros'])

"""# 4.Feature Engineering"""

# now ScreenResolution
df['ScreenResolution'].value_counts()#create one column for touch feature in laptops
df['TouchScreen']=df['ScreenResolution'].apply(lambda x:1 if 'Touchscreen' in x else 0)

df['TouchScreen'].value_counts().plot(kind='bar')
#0=laptop without touchscreen
#1=laptop with touchscreen

"""The code above creates an int type categorical features `TouchScreen` for laptops with and without screen. More like a Boolean Field. 


0 == No TouchScreen
<br>
1 == TouchScreen Device
"""

touchscreen_counts = pd.DataFrame(df['TouchScreen'].value_counts()).reset_index()
touchscreen_counts.columns = ['TouchScreen', 'Count']
# plot the bar chart using plotly express
fig = px.bar(touchscreen_counts, x='TouchScreen', y='Count', color='TouchScreen', 
             labels={'TouchScreen': 'Touch Screen'}, 
             color_discrete_sequence=['#1f77b4', '#ff7f0e'])
fig.show()

#Create one more for IPS functio.
df['IPS']=df['ScreenResolution'].apply(lambda x:1 if'IPS' in x else 0)

df['IPS'].value_counts().plot(kind='bar')

#with price
sns.barplot(x=df['IPS'],y=df['Price_euros'])

#now create 2 more column for resolution from ScreenResolution column
new=df['ScreenResolution'].str.split('x',n=1,expand=True)

new

#2nd column is good but 1st have some problem okay 
df['Height_res']=new[0]
df['Width_res']=new[1]

df['Width_res'].head()

#now remove the strings in X_res column with the help of regular expression
df['Height_res'] = df['Height_res'].str.replace(',','').str.findall(r'(\d+\.?\d+)').apply(lambda x:x[0])

df["Height_res"] = df["Height_res"].astype("int")
df["Width_res"] = df["Width_res"].astype("int")

df.sample()

#create last feature PPI (Pixel per inch) with the help of X_res,Y_res and Inches columns
df['PPI'] = (((df['Height_res']**2) + (df['Width_res']**2))**0.5/df['Inches']).astype('float')

#now drop Inches,X_res,Y_res and ScreenResolution
df.drop(columns=['ScreenResolution','Inches','Width_res','Height_res'],inplace=True)

# now cpu
df['Cpu'].value_counts()

#create more columns aCC. TO THE CATEGORY 
df['Cpu_Name']=df['Cpu'].apply(lambda x:" ".join(x.split()[0:3]))

df.sample(5)

#create function for divide the category
def fetch_processor(text):
    if text == 'Intel Core i7' or text == 'Intel Core i5' or text == 'Intel Core i3':
        return text
    else:
        if text.split()[0] == 'Intel':
            return 'Other Intel Processor'
        else:
            return 'AMD Processor'

df['Cpu_brand']=df['Cpu_Name'].apply(fetch_processor)

df['Cpu_brand'].value_counts().plot(kind='bar')

#with price
sns.barplot(x=df['Cpu_brand'],y=df['Price_euros'])
plt.xticks(rotation='vertical')
plt.show()

#now drop Cpu and Cpu_name Column
df.drop(columns=['Cpu','Cpu_Name'],inplace=True)

df.sample(5)

#Now Ram
df['Ram'].value_counts().plot(kind='bar')

#with price
sns.barplot(x=df['Ram'],y=df['Price_euros'])
plt.xticks(rotation='vertical')
plt.show()

#now  Memory
#df['Memory'].value_counts().plot(kind='bar')
df['Memory'].value_counts()

#transform the Memory column acc. to category

df['Memory'] = df['Memory'].astype(str).replace('\.0', '', regex=True)
df["Memory"] = df["Memory"].str.replace('GB', '')
df["Memory"] = df["Memory"].str.replace('TB', '000')
mem_df = df["Memory"].str.split("+", n = 1, expand = True)

df["first"]= mem_df[0]
df["first"]=df["first"].str.strip()

df["second"]= mem_df[1]

df["Layer1HDD"] = df["first"].apply(lambda x: 1 if "HDD" in x else 0)
df["Layer1SSD"] = df["first"].apply(lambda x: 1 if "SSD" in x else 0)
df["Layer1Hybrid"] = df["first"].apply(lambda x: 1 if "Hybrid" in x else 0)
df["Layer1Flash_Storage"] = df["first"].apply(lambda x: 1 if "Flash Storage" in x else 0)

df['first'] = df['first'].str.replace(r'\D', '')

df["second"].fillna("0", inplace = True)

df["Layer2HDD"] = df["second"].apply(lambda x: 1 if "HDD" in x else 0)
df["Layer2SSD"] = df["second"].apply(lambda x: 1 if "SSD" in x else 0)
df["Layer2Hybrid"] = df["second"].apply(lambda x: 1 if "Hybrid" in x else 0)
df["Layer2Flash_Storage"] = df["second"].apply(lambda x: 1 if "Flash Storage" in x else 0)

df['second'] = df['second'].str.replace(r'\D', '')

df["first"] = df["first"].astype(int)
df["second"] = df["second"].astype(int)

df["HDD"]=(df["first"]*df["Layer1HDD"]+df["second"]*df["Layer2HDD"])
df["SSD"]=(df["first"]*df["Layer1SSD"]+df["second"]*df["Layer2SSD"])
df["Hybrid"]=(df["first"]*df["Layer1Hybrid"]+df["second"]*df["Layer2Hybrid"])
df["Flash_Storage"]=(df["first"]*df["Layer1Flash_Storage"]+df["second"]*df["Layer2Flash_Storage"])

df.drop(columns=['first', 'second', 'Layer1HDD', 'Layer1SSD', 'Layer1Hybrid',
       'Layer1Flash_Storage', 'Layer2HDD', 'Layer2SSD', 'Layer2Hybrid',
       'Layer2Flash_Storage'],inplace=True)

#now drop Memory 
df.drop(columns=['Memory'],inplace=True)

df.sample(5)

#check the Correlation
df.corr()['Price_euros']

df.corr()['Price_euros'].apply(abs).sort_values()

#remove hybrid and Flash_Storage acc. to Correlation
df.drop(columns=['Hybrid','Flash_Storage'],inplace=True)

#now GPU
df['Gpu'].value_counts()

#for brand name only
df['Gpu brand'] = df['Gpu'].apply(lambda x:x.split()[0])

df.sample(4)

df['Gpu brand'].value_counts()

#remove ARM bcz only 1 laptop 
df = df[df['Gpu brand'] != 'ARM']

df['Gpu brand'].value_counts()

sns.barplot(x=df['Gpu brand'],y=df['Price_euros'],estimator=np.median)
plt.xticks(rotation='vertical')
plt.show()

df.drop(columns=['Gpu'],inplace=True)

df.sample(3)

#now OpSys
df['OpSys'].value_counts()

sns.barplot(x=df['OpSys'],y=df['Price_euros'])
plt.xticks(rotation='vertical')
plt.show()

def cat_os(value):
    if value == 'Windows 10' or value == 'Windows 7' or value == 'Windows 10 S':
        return 'Windows'
    elif value == 'macOS' or value == 'Mac OS X':
        return 'Mac'
    else:
        return 'Others/No OS/Linux'

df['os'] = df['OpSys'].apply(cat_os)

df.sample(3)

df.drop(columns=['OpSys'],inplace=True)

df

sns.barplot(x=df['os'],y=df['Price_euros'])
plt.xticks(rotation='vertical')
plt.show()

sns.distplot(df['Weight'])

sns.scatterplot(x=df['Weight'],y=df['Price_euros'])

df.corr()['Price_euros'].apply(abs).sort_values()

sns.heatmap(df.corr())

sns.distplot(df['Price_euros'])

#right skew
#use OneHotEncoder
#log trans.
#use column transformer along with scikit-learn pipeline
sns.distplot(np.log(df['Price_euros']))
#now looking cool

df.dtypes

#right skew
#use OneHotEncoder
#log trans.
#use column transformer along with scikit-learn pipeline
sns.distplot(np.log(df['Price_euros']))
#now looking cool

X = df.drop(columns=['Price_euros'])
y = df['Price_euros']

X

y

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score,mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.15)

X_train

X_test

step1 = ColumnTransformer(transformers=[
    ('col_tnf',OneHotEncoder(sparse=False,drop='first'),[0,1,7,10,11])
],remainder='passthrough')

step2 = RandomForestRegressor(n_estimators=100,
                              random_state=3,
                              max_samples=0.5,
                              max_features=0.75,
                              max_depth=15)

pipe = Pipeline([
    ('step1',step1),
    ('step2',step2)
])

pipe.fit(X_train,y_train)

y_pred = pipe.predict(X_test)

print('R2 score',r2_score(y_test,y_pred))
print('MAE',mean_absolute_error(y_test,y_pred))

X_test

# Converting X_test is a Pandas DataFrame
X_test.to_csv('testing.csv', index=False)



# step2 = RandomForestRegressor(n_estimators=100,
#                               random_state=3,
#                               max_samples=0.5,
#                               max_features=0.75,
#                               max_depth=15)

# pipe = Pipeline([
#     ('step1',step1),
#     ('step2',step2)
# ])

# pipe.fit(X_train,y_train)

pipe.score(X_test, y_test)

import plotly.graph_objects as go

fig = go.Figure()

fig.add_trace(
    go.Scatter(
        x=y_pred,
        y=y_test,
        mode='markers'
    )
)

fig.add_trace(
    go.Scatter(
        x=[0, 6000],
        y=[0, 6000],
        mode='lines',
        line=dict(color='red')
    )
)

fig.update_layout(
    width=800,
    height=600,
    xaxis_title='Predicted Values',
    yaxis_title='Actual Values'
)

fig.show()

"""Saving Trained Model"""

import pickle

filename = "trained_model.sav"

pickle.dump(pipe, open(filename, 'wb'))

"""Loading saved Model"""

# Loading model 
loaded_model = pickle.load(open('trained_model.sav', 'rb'))







